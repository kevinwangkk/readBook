## 	net

##### 1. 数据两次拷贝

​	网卡 拷贝 -> 内核  拷贝->用户空间   [2次拷贝]

**note:** 1.  IP首部中所有的数值都以网络字节序存储(大端序)

​	

数据载体 skb  ( socket buf ) [ 每一层用指针来表示  skb贯穿整个网络协议栈 ]
​	struct sk_buff




MTU - Maximum Transmission Unit  帧  一种通信协议的某一层上面所能通过的最大数据包大小（以字节为单位）



##### 2. 数据传输框架

网卡驱动层  net_device [ register_netdev ( ) ]



​	(skb_protocol 设置协议 决定之后的处理方式)

​        两种途径[把网卡驱动数据往上一层传输] : (1)netif_rx   (2)netif_rx_ni



​         packet  分包 [netif_receive_skb]  

​               IP 层  IPv4 IPv6

​                  TCP UDP

​                     socket

​                      API

##### 细化

1. 网卡驱动层  struct **net_device**

    1. dev=**alloc_ethrdev ()**  //分配一个net_device

    2. 网卡驱动的操作 :  struct **net_device_ops** 

    3. struct **ethtool_ops**

    4. **register_netdev**(dev) //注册网络设备

2. 网卡驱动接收到数据处理 

    1. **netdev_alloc_skb**()  // 分配skb  之后从网卡驱动拷贝到内核空间

    2. skb->protocol   //设置协议类型 [依据网卡设置协议类型, 决定以后处理方式]

       ​	skb->protocol =  **eth_type_trans**(skb, dev)  //以太网通用处理 [ ETH_P_802_3 ]

    3. dev->stats.rx_packets++   //分包的计数增加

      	4. dev->stats.rx_bytes += cf->can_dlc  //字节数增加 [can设备]

       ​	struct **net_device_stats**   //网络设备状态统计量

       ​		unsigned long **rx_packets**;

       ​		unsigned long **tx_packets**;

       ​		......

3. netif_rx() 向上传输
   ​    1. **netif_rx**()  ---> **netif_rx_internal**() 
   ​    2. linux RPS  流量控制
   ​    3. 把分包放到队列里 **enqueue_to_backlog**() //积压队列
   ​       1. sd = &**per_cpu**(softnet_data, cpu);  //每CPU
   ​       2. qlen = **skb_queue_len**(&sd->input_pkt_queue);  //这个cpu下的队列长度[封包个数]
   ​       3. if (qlen <= netdev_max_backlog && !**skb_flow_limit**(skb, qlen))  //sd->flow_limit 每cpu的限制流控
   ​       4. 

